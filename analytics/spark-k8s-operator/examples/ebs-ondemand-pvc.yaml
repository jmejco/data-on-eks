# NOTE: This example requires the following prerequisites before executing the jobs
# 1. Ensure spark-team-a name space exists
# 2. Ensure YuniKorn Helm Chart deployed in your Cluster
# 3. replace <ENTER_YOUR_BUCKET> with your bucket name
# 4. Ensure you run "analytics/spark-k8s-operator/spark-samples/tpcds-benchmark-data-generation-1t.yaml" to generate the INPUT data in S3 bucket and update INPUT argument(      "s3a://<ENTER_YOUR_S3_BUCKET>/TPCDS-TEST-1T/catalog_sales/") path in the below yaml

# This example supports the following features
  # Support shuffle data recovery on the reused PVCs (SPARK-35593)
  # Support driver-owned on-demand PVC (SPARK-35182)
# WARNING: spark-operator cluster role is missing a 'persistenvolumeclaims' permission. Ensure you add this permission to spark-operator cluster role

---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: catalog-sales-test1
  namespace: spark-team-a
  labels:
    app: catalog-sales
    applicationId: catalog-sales-yunikorn
    queue: root.test
spec:
#  batchScheduler: "yunikorn"
  type: Python
  sparkVersion: "3.2.1"
  pythonVersion: "3"
  mode: cluster
  # Public docker image used from datamechanics/spark:3.2.1-hadoop-3.3.1-java-11-scala-2.12-python-3.8-latest and pushed to ECR repo
  image: "public.ecr.aws/r1l5w1y9/spark-operator:3.2.1-hadoop-3.3.1-java-11-scala-2.12-python-3.8-latest"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "s3a://<ENTER_YOUR_S3_BUCKET>/spark-catalog-sales.py"  # MainFile is the path to a bundled JAR, Python, or R file of the application
  arguments:
    - "s3a://<ENTER_YOUR_S3_BUCKET>/TPCDS-TEST-1T/catalog_sales/" # Copy your Input parquet sample data before executing the job
    - "s3a://<ENTER_YOUR_S3_BUCKET>/TPCDS-OUTPUT/catalog_sales/"
  hadoopConf:
    "fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider" # or  "com.amazonaws.auth.InstanceProfileCredentialsProvider"
    "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "mapreduce.fileoutputcommitter.algorithm.version": "2"
  sparkConf:
    "spark.speculation": "false"
    "spark.network.timeout": "2400"
    "spark.hadoop.fs.s3a.connection.timeout": "1200000"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.maximum": "200"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.fs.s3a.readahead.range": "256K"
    "spark.hadoop.fs.s3a.input.fadvise": "random"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"

    # YuniKorn Config
    "spark.kubernetes.driver.label.queue": "root.test"
    "spark.kubernetes.driver.label.yunikorn.apache.org/username": "vara"
    "spark.kubernetes.executor.label.queue": "root.test"
    "spark.kubernetes.executor.label.yunikorn.apache.org/username": "vara"

    # EBS Dynamic PVC Config
    # You can mount a dynamically-created persistent volume claim per executor by using OnDemand as a claim name and storageClass and sizeLimit options like the following. This is useful in case of Dynamic Allocation.
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName": "OnDemand"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass": "gp2"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit": "500Gi"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path": "/data1"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly": "false"

    "spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName": "OnDemand"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass": "gp2"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit": "500Gi"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path": "/data1"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly": "false"

    # Support shuffle data recovery on the reused PVCs (SPARK-35593)
    "spark.kubernetes.driver.ownPersistentVolumeClaim": "true" # If true, driver pod becomes the owner of on-demand persistent volume claims instead of the executor pods
    "spark.kubernetes.driver.reusePersistentVolumeClaim": "true" # If true, driver pod tries to reuse driver-owned on-demand persistent volume claims of the deleted executor pods if exists. This can be useful to reduce executor pod creation delay by skipping persistent volume creations. Note that a pod in `Terminating` pod status is not a deleted pod by definition and its resources including persistent volume claims are not reusable yet. Spark will create new persistent volume claims when there exists no reusable one. In other words, the total number of persistent volume claims can be larger than the number of running executors sometimes. This config requires spark.kubernetes.driver.ownPersistentVolumeClaim=true.

    #Event logs
    "spark.local.dir": "/data1"
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://<ENTER_YOUR_S3_BUCKET>/spark-event-logs"

  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20

  driver:
    podSecurityContext:
      fsGroup: 185
    initContainers:
      - name: volume-permissions
        image: public.ecr.aws/y4g4v0z7/busybox
        command: [ 'sh', '-c', 'chown -R 185 /data1' ]
        volumeMounts:
          - mountPath: "/data1"
            name: "spark-local-dir-1"
    cores: 1
    coreLimit: "1200m"
    memory: "10g"
    memoryOverhead: "4g"
    serviceAccount: spark-team-a
    labels:
      version: 3.2.1
    nodeSelector:
      "NodeGroupType": "spark"
  executor:
    podSecurityContext:
      fsGroup: 185
    initContainers:
      - name: volume-permissions
        image: public.ecr.aws/y4g4v0z7/busybox
        command: [ 'sh', '-c', 'chown -R 185 /data1' ]
        volumeMounts:
          - mountPath: "/data1"
            name: "spark-local-dir-1"
    cores: 1
    coreLimit: "1200m"
    instances: 4
    memory: "10g"
    memoryOverhead: "4g"
    serviceAccount: spark-team-a
    labels:
      version: 3.2.1
    nodeSelector:
      "NodeGroupType": "spark"
